# Documentation Scraper with Vector DB & MCP Server

A powerful documentation scraping and search system that uses vector embeddings to enable semantic search capabilities across framework documentation.

## ğŸŒŸ Features

- ğŸ•·ï¸ **Web Scraping**: Efficiently crawls and extracts content from framework documentation
- ğŸ§¹ **Text Processing**: Cleans and normalizes HTML content while preserving semantic structure
- ğŸ§© **Smart Chunking**: Splits documentation into meaningful, context-aware chunks
- ğŸ§  **Vector Embeddings**: Generates embeddings using state-of-the-art models
- ğŸ—„ï¸ **Vector Database**: Stores and indexes embeddings for fast similarity search
- ğŸ”Œ **MCP Integration**: Seamless integration with MCP server for documentation search
- ğŸ“Š **CLI Interface**: Enables easy local testing and usage

## ğŸ› ï¸ Technology Stack

- **Language**: Python 3.11+
- **Package Management**: uv (Modern Python package installer)
- **Web Scraping**: BeautifulSoup4, Requests
- **Vector Embeddings**: OpenAI/HuggingFace
- **Database**: PostgreSQL with pgvector
- **Testing**: Pytest
- **Containerization**: Docker
- **Configuration**: python-dotenv
- **Logging**: Built-in Python logging with structured output

## ğŸ“‹ Prerequisites

- Python 3.11 or higher
- PostgreSQL with pgvector extension
- Docker (optional)
- OpenAI API key (or alternative embedding provider)
- uv package installer (recommended)

## ğŸš€ Getting Started

1. Clone the repository:
   ```bash
   git clone <repository-url>
   cd docu_ai
   ```

2. Set up the virtual environment:
   ```bash
   python -m venv .venv
   source .venv/bin/activate  # On Windows: .venv\Scripts\activate
   ```

3. Install dependencies using uv (recommended):
   ```bash
   pip install uv
   uv pip install -r requirements.txt
   ```
   Or using pip:
   ```bash
   pip install -r requirements.txt
   ```

4. Set up environment variables:
   ```bash
   cp .env.sample .env
   # Edit .env with your configuration
   ```

5. Run the CLI:
   ```bash
   python -m src.cli.main
   ```

## ğŸ—ï¸ Project Structure

```
docu_ai/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scraper/     # Web scraping and HTML processing
â”‚   â”œâ”€â”€ db/          # Database operations and vector storage
â”‚   â”œâ”€â”€ cli/         # Command-line interface
â”‚   â”œâ”€â”€ config/      # Configuration management
â”‚   â””â”€â”€ main.py      # Application entry point
â”œâ”€â”€ tests/           # Test suites mirroring src structure
â”œâ”€â”€ docker/          # Docker configuration files
â”œâ”€â”€ docs/           # Additional documentation
â”œâ”€â”€ logs/           # Application logs
â””â”€â”€ .env.sample     # Environment variables template
```

## ğŸ”§ Configuration

The project uses environment variables for configuration. Copy `.env.sample` to `.env` and configure:

```env
# Database Configuration
DB_HOST=localhost
DB_PORT=5432
DB_NAME=docuai
DB_USER=postgres
DB_PASSWORD=your_password

# OpenAI Configuration
OPENAI_API_KEY=your_api_key

# Scraping Configuration
SCRAPE_DELAY=2
MAX_RETRIES=3
```

## ğŸ§ª Running Tests

```bash
# Run all tests
pytest

# Run specific test file
pytest tests/test_scraper.py

# Run with coverage
pytest --cov=src tests/
```

## ğŸ³ Docker Deployment

Coming soon...

## ğŸ“ Logging

Logs are stored in the `logs/` directory with the following structure:
- `app.log`: General application logs
- `error.log`: Error-specific logs
- `scraper.log`: Scraping-specific logs

## ğŸ¤ Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- OpenAI for embedding models
- pgvector contributors
- uv team for the modern Python package installer
